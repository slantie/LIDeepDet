{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f86a7c02",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a27ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "# --- Input Paths ---\n",
    "# IMPORTANT: Update this path to your Celeb-DF dataset location\n",
    "BASE_DIR = \"D://Drishtiksha//Data//Celeb-DF-v2\"\n",
    "REAL_VIDEOS_PATH = os.path.join(BASE_DIR, \"real\")\n",
    "FAKE_VIDEOS_PATH = os.path.join(BASE_DIR, \"fake\")\n",
    "\n",
    "# --- Output Path ---\n",
    "# This directory will be created to store the processed features\n",
    "PREPROCESSED_FEATURES_PATH = \"Features\"\n",
    "\n",
    "# --- Preprocessing Settings ---\n",
    "MAX_VIDEOS_PER_FOLDER = 51\n",
    "MAX_FRAMES_PER_VIDEO = 100\n",
    "IMG_SIZE = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64833c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_illumination_map_paper(face_crop):\n",
    "    \"\"\"\n",
    "    CORRECTED IMPLEMENTATION.\n",
    "    Creates an illumination map that aligns with the paper's goal: \"preserve the\n",
    "    image's overall structure and smooth texture details\". This is achieved robustly\n",
    "    using a Guided Filter, an edge-preserving smoothing technique.\n",
    "    \"\"\"\n",
    "    if face_crop is None: return None\n",
    "    \n",
    "    # 1. Get initial illumination map M_hat (from Equation 2)\n",
    "    m_hat = np.max(face_crop, axis=-1).astype(np.float32) / 255.0\n",
    "\n",
    "    # 2. Use the original image as a guide to preserve structure\n",
    "    # The guided filter will smooth m_hat, but not across the edges present in the guide\n",
    "    guide_image = cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 3. Create and apply the guided filter\n",
    "    # Radius and epsilon are key parameters. A larger radius means more smoothing.\n",
    "    radius = 32\n",
    "    epsilon = 0.01\n",
    "    guided_filter = cv2.ximgproc.createGuidedFilter(guide=guide_image, radius=radius, eps=epsilon)\n",
    "    M = guided_filter.filter(src=m_hat)\n",
    "    \n",
    "    # Normalize for saving and visualization\n",
    "    smoothed_map = cv2.normalize(M, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    smoothed_map = np.uint8(smoothed_map)\n",
    "    return cv2.cvtColor(smoothed_map, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "\n",
    "def extract_face_material_map_paper(face_crop, mask_size=5):\n",
    "    \"\"\"\n",
    "    Creates a face material map using the Pattern of Local Gravitational Force\n",
    "    (PLGF) descriptor as defined in Equations (5-7) of the paper.\n",
    "    The paper's text states the magnitude is used for texture, so this is correct.\n",
    "    \"\"\"\n",
    "    if face_crop is None: return None\n",
    "    gray_face = cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY).astype(np.float64)\n",
    "    radius = mask_size // 2\n",
    "    y, x = np.mgrid[-radius:radius+1, -radius:radius+1]\n",
    "    epsilon = 1e-12\n",
    "    denominator = x**2 + y**2 + epsilon\n",
    "    kernel_tx = (np.cos(np.arctan2(y, x))) / denominator\n",
    "    kernel_ty = (np.sin(np.arctan2(y, x))) / denominator\n",
    "    kernel_tx[radius, radius] = 0\n",
    "    kernel_ty[radius, radius] = 0\n",
    "    gx = convolve2d(gray_face, kernel_tx, mode='same', boundary='symm')\n",
    "    gy = convolve2d(gray_face, kernel_ty, mode='same', boundary='symm')\n",
    "    magnitude = np.sqrt(gx**2 + gy**2)\n",
    "    material_map = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    material_map = np.uint8(material_map)\n",
    "    return cv2.cvtColor(material_map, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "def estimate_light_direction(face_crop):\n",
    "    \"\"\"\n",
    "    Estimates the 2D light direction vector based on the Lambertian model\n",
    "    (Paper Section 3.4), assuming the brightest region of the face points\n",
    "    towards the light source. This remains a robust proxy.\n",
    "    \"\"\"\n",
    "    if face_crop is None: return np.array([0.0, 0.0])\n",
    "    gray_face = cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY)\n",
    "    _, _, _, max_loc = cv2.minMaxLoc(gray_face)\n",
    "    center_x, center_y = gray_face.shape[1] // 2, gray_face.shape[0] // 2\n",
    "    vec = np.array([max_loc[0] - center_x, max_loc[1] - center_y], dtype=np.float32)\n",
    "    norm = np.linalg.norm(vec)\n",
    "    if norm > 0: vec /= norm\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb11dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path, output_dir, face_detector):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened(): return {}\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if total_frames < 1:\n",
    "        cap.release()\n",
    "        return {}\n",
    "    indices = np.linspace(0, total_frames - 1, MAX_FRAMES_PER_VIDEO, dtype=int) if total_frames > MAX_FRAMES_PER_VIDEO else np.arange(total_frames)\n",
    "    features = {'frames': [], 'illum_maps': [], 'material_maps': [], 'light_vectors': []}\n",
    "    saved_frame_count = 0\n",
    "    \n",
    "    for frame_idx in indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: continue\n",
    "\n",
    "        # --- FACE DETECTION CHANGE ---\n",
    "        # 1. Convert BGR (OpenCV default) to RGB (MTCNN expects this)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # 2. Use the MTCNN detector\n",
    "        faces = face_detector.detect_faces(frame_rgb)\n",
    "\n",
    "        if len(faces) > 0:\n",
    "            # 3. Get the bounding box from the most confident detection\n",
    "            best_face = sorted(faces, key=lambda f: f['confidence'], reverse=True)[0]\n",
    "            x, y, w, h = best_face['box']\n",
    "            \n",
    "            # Ensure coordinates are positive before cropping\n",
    "            x, y = max(0, x), max(0, y)\n",
    "            \n",
    "            # Use the original BGR frame for cropping\n",
    "            face_crop = frame[y:y+h, x:x+w]\n",
    "            \n",
    "            # Handle cases where the face crop is empty\n",
    "            if face_crop.size == 0:\n",
    "                continue\n",
    "\n",
    "            face_crop_resized = cv2.resize(face_crop, IMG_SIZE)\n",
    "            \n",
    "            # --- The rest of the pipeline is the same ---\n",
    "            illum_map = extract_illumination_map_paper(face_crop_resized)\n",
    "            material_map = extract_face_material_map_paper(face_crop_resized)\n",
    "            light_vector = estimate_light_direction(face_crop_resized)\n",
    "            \n",
    "            fid = f\"{saved_frame_count:04d}\"\n",
    "            cv2.imwrite(os.path.join(output_dir, f\"{fid}_frame.png\"), face_crop_resized)\n",
    "            if illum_map is not None: cv2.imwrite(os.path.join(output_dir, f\"{fid}_illum.png\"), illum_map)\n",
    "            if material_map is not None: cv2.imwrite(os.path.join(output_dir, f\"{fid}_material.png\"), material_map)\n",
    "            np.save(os.path.join(output_dir, f\"{fid}_lightvec.npy\"), light_vector)\n",
    "            \n",
    "            features['frames'].append(face_crop_resized)\n",
    "            features['illum_maps'].append(illum_map)\n",
    "            features['material_maps'].append(material_map)\n",
    "            features['light_vectors'].append(light_vector)\n",
    "            saved_frame_count += 1\n",
    "    cap.release()\n",
    "    return features\n",
    "\n",
    "def visualize_preprocessing_steps(features):\n",
    "    if not features or not features['frames']: return\n",
    "    frame, illum_map, material_map, light_vector = features['frames'][0], features['illum_maps'][0], features['material_maps'][0], features['light_vectors'][0]\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    fig.suptitle('Preprocessing Visualization (Corrected Implementation)', fontsize=16)\n",
    "    axes[0].imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)); axes[0].set_title('1. Original Face Crop'); axes[0].axis('off')\n",
    "    axes[1].imshow(illum_map); axes[1].set_title('2. Illumination Map'); axes[1].axis('off')\n",
    "    cx, cy = IMG_SIZE[0] // 2, IMG_SIZE[1] // 2\n",
    "    axes[1].arrow(cx, cy, light_vector[0] * 50, light_vector[1] * 50, head_width=10, head_length=10, fc='r', ec='r')\n",
    "    axes[2].imshow(material_map, cmap='gray'); axes[2].set_title('3. Face Material Map'); axes[2].axis('off')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa330a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_preprocessing():\n",
    "    \"\"\"Main function to run the entire preprocessing pipeline.\"\"\"\n",
    "    print(\"Starting preprocessing...\")\n",
    "\n",
    "    os.makedirs(PREPROCESSED_FEATURES_PATH, exist_ok=True)\n",
    "    progress_file = os.path.join(PREPROCESSED_FEATURES_PATH, '_progress.txt')\n",
    "    completed_videos = set()\n",
    "    try:\n",
    "        with open(progress_file, 'r') as f:\n",
    "            completed_videos = {line.strip() for line in f}\n",
    "        print(f\"Found {len(completed_videos)} videos already processed. Resuming.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Starting a new preprocessing run.\")\n",
    "        \n",
    "    # --- INITIALIZATION CHANGE ---\n",
    "    print(\"Initializing MTCNN face detector...\")\n",
    "    face_detector = MTCNN()\n",
    "    \n",
    "    videos_to_process = []\n",
    "    for category, path in {'real': REAL_VIDEOS_PATH, 'fake': FAKE_VIDEOS_PATH}.items():\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"Warning: Directory not found, skipping: {path}\")\n",
    "            continue\n",
    "        \n",
    "        video_files = [f for f in os.listdir(path) if f.endswith(('.mp4', '.avi', '.mov'))]\n",
    "        if MAX_VIDEOS_PER_FOLDER is not None: video_files = video_files[:MAX_VIDEOS_PER_FOLDER]\n",
    "\n",
    "        for video_name in video_files:\n",
    "            video_id = os.path.splitext(video_name)[0]\n",
    "            video_unique_id = f\"{category}/{video_id}\"\n",
    "            if video_unique_id not in completed_videos:\n",
    "                videos_to_process.append({\n",
    "                    'id': video_unique_id,\n",
    "                    'path': os.path.join(path, video_name),\n",
    "                    'output_dir': os.path.join(PREPROCESSED_FEATURES_PATH, category, video_id),\n",
    "                    'category': category\n",
    "                })\n",
    "\n",
    "    if not videos_to_process:\n",
    "        print(\"All specified videos have already been processed. Nothing to do.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Total new videos to process: {len(videos_to_process)}\")\n",
    "    first_real_video_features = None\n",
    "    \n",
    "    with open(progress_file, 'a') as progress_log:\n",
    "        for video_info in tqdm(videos_to_process, desc=\"Processing Videos\"):\n",
    "            os.makedirs(video_info['output_dir'], exist_ok=True)\n",
    "            features = process_video(video_info['path'], video_info['output_dir'], face_detector)\n",
    "            progress_log.write(f\"{video_info['id']}\\n\"); progress_log.flush()\n",
    "            if video_info['category'] == 'real' and first_real_video_features is None:\n",
    "                first_real_video_features = features\n",
    "\n",
    "    print(\"\\nPreprocessing complete.\")\n",
    "    if first_real_video_features:\n",
    "        print(\"Displaying visualization for the first real video processed in this session...\")\n",
    "        visualize_preprocessing_steps(first_real_video_features)\n",
    "    else:\n",
    "        print(\"No new real videos were processed in this session, skipping visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70118757",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_preprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde1b2b4",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8e7f15",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbc479e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import timm\n",
    "import glob\n",
    "import json\n",
    "import wandb\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "\n",
    "class Config:\n",
    "    PREPROCESSED_DATA_DIR = \"Features\"\n",
    "    OUTPUT_DIR = \"Outputs\"\n",
    "    WANDB_PROJECT_NAME = \"LIDeepDet\"\n",
    "    WANDB_RUN_NAME = \"run-100-videos-weight-decay\"\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    BATCH_SIZE = 4\n",
    "    LEARNING_RATE = 1e-5\n",
    "    EPOCHS = 5\n",
    "    IMG_SIZE = (224, 224)\n",
    "    VIT_MODEL_NAME = 'vit_base_patch16_224'\n",
    "    EMBED_DIM = 768\n",
    "    RESUME_CHECKPOINT = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36efa3f1",
   "metadata": {},
   "source": [
    "#### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182e4479",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepfakeDataset(Dataset):\n",
    "    # ... (This class is unchanged from the last robust version)\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.transform = transform\n",
    "        real_videos = [d for d in glob.glob(os.path.join(root_dir, 'real', '*')) if os.path.isdir(d)]\n",
    "        fake_videos = [d for d in glob.glob(os.path.join(root_dir, 'fake', '*')) if os.path.isdir(d)]\n",
    "        all_folders = [(d, 0) for d in real_videos] + [(d, 1) for d in fake_videos]\n",
    "        self.video_folders = [(path, label) for path, label in all_folders if len(glob.glob(os.path.join(path, '*_frame.png'))) > 0]\n",
    "        self.num_real = len(real_videos)\n",
    "        self.num_fake = len(fake_videos)\n",
    "        if len(all_folders) != len(self.video_folders): print(f\"Warning: Filtered out {len(all_folders) - len(self.video_folders)} empty video directories.\")\n",
    "    def __len__(self): return len(self.video_folders)\n",
    "    def __getitem__(self, idx):\n",
    "        video_dir, label = self.video_folders[idx]\n",
    "        frame_files = glob.glob(os.path.join(video_dir, '*_frame.png'))\n",
    "        if not frame_files: return self.__getitem__((idx + 1) % len(self))\n",
    "        random_frame_path = np.random.choice(frame_files)\n",
    "        frame_id = os.path.basename(random_frame_path).split('_')[0]\n",
    "        rgb_path = random_frame_path\n",
    "        illum_path = os.path.join(video_dir, f\"{frame_id}_illum.png\")\n",
    "        material_path = os.path.join(video_dir, f\"{frame_id}_material.png\")\n",
    "        if not all(os.path.exists(p) for p in [rgb_path, illum_path, material_path]): return self.__getitem__((idx + 1) % len(self))\n",
    "        rgb_img = cv2.cvtColor(cv2.imread(rgb_path), cv2.COLOR_BGR2RGB)\n",
    "        illum_img = cv2.cvtColor(cv2.imread(illum_path), cv2.COLOR_BGR2RGB)\n",
    "        material_img = cv2.cvtColor(cv2.imread(material_path), cv2.COLOR_BGR2RGB)\n",
    "        if rgb_img is None or illum_img is None or material_img is None: return self.__getitem__((idx + 1) % len(self))\n",
    "        if self.transform:\n",
    "            rgb_img, illum_img, material_img = self.transform(rgb_img), self.transform(illum_img), self.transform(material_img)\n",
    "        return (rgb_img, illum_img, material_img), torch.tensor(label, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b51c7a5",
   "metadata": {},
   "source": [
    "#### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e48c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        attn_output, _ = self.multihead_attn(query=query, key=key, value=value)\n",
    "        return attn_output\n",
    "\n",
    "class LIDeepDet(nn.Module):\n",
    "    def __init__(self, vit_model_name, embed_dim, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.backbone_rgb = timm.create_model(vit_model_name, pretrained=pretrained, num_classes=0)\n",
    "        self.backbone_illum = timm.create_model(vit_model_name, pretrained=pretrained, num_classes=0)\n",
    "        self.backbone_material = timm.create_model(vit_model_name, pretrained=pretrained, num_classes=0)\n",
    "        self.cross_attention = CrossAttention(embed_dim)\n",
    "        self.classifier = nn.Sequential(nn.LayerNorm(embed_dim * 6), nn.Linear(embed_dim * 6, embed_dim), nn.GELU(), nn.Linear(embed_dim, 1))\n",
    "\n",
    "    def forward(self, rgb_img, illum_img, material_img):\n",
    "        f_rgb = self.backbone_rgb.forward_features(rgb_img)[:, 0].unsqueeze(1)\n",
    "        f_illum = self.backbone_illum.forward_features(illum_img)[:, 0].unsqueeze(1)\n",
    "        f_material = self.backbone_material.forward_features(material_img)[:, 0].unsqueeze(1)\n",
    "        a_ri = self.cross_attention(f_rgb, f_illum, f_illum)\n",
    "        a_rm = self.cross_attention(f_rgb, f_material, f_material)\n",
    "        a_ir = self.cross_attention(f_illum, f_rgb, f_rgb)\n",
    "        a_im = self.cross_attention(f_illum, f_material, f_material)\n",
    "        a_mr = self.cross_attention(f_material, f_rgb, f_rgb)\n",
    "        a_mi = self.cross_attention(f_material, f_illum, f_illum)\n",
    "        fused_features = torch.cat([a_ri, a_rm, a_ir, a_im, a_mr, a_mi], dim=-1).squeeze(1)\n",
    "        return self.classifier(fused_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c7405f",
   "metadata": {},
   "source": [
    "#### Training & Evaluation Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0942de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss, all_preds, all_labels = 0.0, [], []\n",
    "    for (rgb, illum, material), labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        rgb, illum, material = rgb.to(device), illum.to(device), material.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(rgb, illum, material)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        all_preds.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics for the entire epoch\n",
    "    all_preds, all_labels = np.array(all_preds), np.array(all_labels)\n",
    "    return {\n",
    "        \"loss\": total_loss / len(loader),\n",
    "        \"accuracy\": np.mean((all_preds > 0.5) == all_labels),\n",
    "        \"auc\": roc_auc_score(all_labels, all_preds)\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss, all_preds, all_labels = 0.0, [], []\n",
    "    for (rgb, illum, material), labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        rgb, illum, material = rgb.to(device), illum.to(device), material.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "        outputs = model(rgb, illum, material)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        all_preds.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_preds, all_labels = np.array(all_preds), np.array(all_labels)\n",
    "    # Handle case where validation set might have only one class\n",
    "    if len(np.unique(all_labels)) < 2:\n",
    "        auc = 0.5 # Can't compute AUC with only one class, default to 0.5\n",
    "    else:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / len(loader),\n",
    "        \"accuracy\": np.mean((all_preds > 0.5) == all_labels),\n",
    "        \"auc\": auc,\n",
    "        \"predictions\": all_preds, # Return raw predictions for logging\n",
    "        \"labels\": all_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb569c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import timm\n",
    "import wandb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# ==============================================================================\n",
    "class Config:\n",
    "    PREPROCESSED_DATA_DIR = \"Features\"\n",
    "    OUTPUT_DIR = \"Outputs\"\n",
    "    WANDB_PROJECT_NAME = \"LIDeepDet\"\n",
    "    WANDB_RUN_NAME = \"run-100-videos-augmented\"\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    BATCH_SIZE = 8\n",
    "    LEARNING_RATE = 1e-5 # Lowered for more stable fine-tuning\n",
    "    EPOCHS = 5\n",
    "    VIT_MODEL_NAME = 'vit_base_patch16_224'\n",
    "    EMBED_DIM = 768\n",
    "    RESUME_CHECKPOINT = None\n",
    "    IMG_SIZE = (224, 224)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. DATASET & MODEL (Unchanged)\n",
    "# ==============================================================================\n",
    "# The DeepfakeDataset and LIDeepDet classes are correct and do not need changes.\n",
    "# They are included here to make the script standalone.\n",
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.transform = transform\n",
    "        real_videos = [d for d in glob.glob(os.path.join(root_dir, 'real', '*')) if os.path.isdir(d)]\n",
    "        fake_videos = [d for d in glob.glob(os.path.join(root_dir, 'fake', '*')) if os.path.isdir(d)]\n",
    "        all_folders = [(d, 0) for d in real_videos] + [(d, 1) for d in fake_videos]\n",
    "        self.video_folders = [(p, l) for p, l in all_folders if len(glob.glob(os.path.join(p, '*_frame.png'))) > 0]\n",
    "        self.num_real, self.num_fake = len(real_videos), len(fake_videos)\n",
    "        if len(all_folders) != len(self.video_folders): print(f\"Warning: Filtered {len(all_folders) - len(self.video_folders)} empty video directories.\")\n",
    "    def __len__(self): return len(self.video_folders)\n",
    "    def __getitem__(self, idx):\n",
    "        video_dir, label = self.video_folders[idx]\n",
    "        frame_files = glob.glob(os.path.join(video_dir, '*_frame.png'))\n",
    "        if not frame_files: return self.__getitem__((idx + 1) % len(self))\n",
    "        frame_path = np.random.choice(frame_files)\n",
    "        frame_id = os.path.basename(frame_path).split('_')[0]\n",
    "        paths = [frame_path, os.path.join(video_dir, f\"{frame_id}_illum.png\"), os.path.join(video_dir, f\"{frame_id}_material.png\")]\n",
    "        if not all(os.path.exists(p) for p in paths): return self.__getitem__((idx + 1) % len(self))\n",
    "        images = [cv2.cvtColor(cv2.imread(p), cv2.COLOR_BGR2RGB) for p in paths]\n",
    "        if any(img is None for img in images): return self.__getitem__((idx + 1) % len(self))\n",
    "        if self.transform: images = [self.transform(img) for img in images]\n",
    "        return tuple(images), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "class LIDeepDet(nn.Module):\n",
    "    def __init__(self, vit_model_name, embed_dim, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.backbone_rgb = timm.create_model(vit_model_name, pretrained=pretrained, num_classes=0)\n",
    "        self.backbone_illum = timm.create_model(vit_model_name, pretrained=pretrained, num_classes=0)\n",
    "        self.backbone_material = timm.create_model(vit_model_name, pretrained=pretrained, num_classes=0)\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim, 8, batch_first=True)\n",
    "        self.classifier = nn.Sequential(nn.LayerNorm(embed_dim * 6), nn.Linear(embed_dim * 6, embed_dim), nn.GELU(), nn.Linear(embed_dim, 1))\n",
    "    def forward(self, rgb, illum, material):\n",
    "        f_rgb, f_illum, f_mat = [b.forward_features(x)[:, 0].unsqueeze(1) for b, x in zip((self.backbone_rgb, self.backbone_illum, self.backbone_material), (rgb, illum, material))]\n",
    "        a_ri, _ = self.cross_attention(f_rgb, f_illum, f_illum)\n",
    "        a_rm, _ = self.cross_attention(f_rgb, f_mat, f_mat)\n",
    "        a_ir, _ = self.cross_attention(f_illum, f_rgb, f_rgb)\n",
    "        a_im, _ = self.cross_attention(f_illum, f_mat, f_mat)\n",
    "        a_mr, _ = self.cross_attention(f_mat, f_rgb, f_rgb)\n",
    "        a_mi, _ = self.cross_attention(f_mat, f_illum, f_illum)\n",
    "        fused = torch.cat([a_ri, a_rm, a_ir, a_im, a_mr, a_mi], dim=-1).squeeze(1)\n",
    "        return self.classifier(fused)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. HELPER FUNCTIONS (ENHANCED METRICS & PLOTTING)\n",
    "# ==============================================================================\n",
    "def get_metrics(labels, preds):\n",
    "    \"\"\"Calculates a dictionary of metrics.\"\"\"\n",
    "    binary_preds = (preds > 0.5).astype(int)\n",
    "    metrics = {\n",
    "        \"accuracy\": np.mean(binary_preds == labels),\n",
    "        \"precision\": precision_score(labels, binary_preds, zero_division=0),\n",
    "        \"recall\": recall_score(labels, binary_preds, zero_division=0),\n",
    "        \"f1_score\": f1_score(labels, binary_preds, zero_division=0)\n",
    "    }\n",
    "    if len(np.unique(labels)) > 1:\n",
    "        metrics[\"auc\"] = roc_auc_score(labels, preds)\n",
    "    else:\n",
    "        metrics[\"auc\"] = 0.5 # Default AUC if only one class is present\n",
    "    return metrics\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss, all_preds, all_labels = 0.0, [], []\n",
    "    for images, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        images = [img.to(device) for img in images]\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(*images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        all_preds.extend(torch.sigmoid(outputs).detach().cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    metrics = get_metrics(np.array(all_labels), np.array(all_preds))\n",
    "    metrics['loss'] = total_loss / len(loader)\n",
    "    return metrics\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss, all_preds, all_labels = 0.0, [], []\n",
    "    for images, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        images = [img.to(device) for img in images]\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "        outputs = model(*images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        all_preds.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    metrics = get_metrics(np.array(all_labels), np.array(all_preds))\n",
    "    metrics['loss'] = total_loss / len(loader)\n",
    "    metrics['predictions'] = np.array(all_preds)\n",
    "    metrics['labels'] = np.array(all_labels)\n",
    "    return metrics\n",
    "\n",
    "def log_diagnostic_plots(val_metrics, epoch):\n",
    "    \"\"\"Generates and logs confusion matrix and prediction histogram to W&B.\"\"\"\n",
    "    labels, preds = val_metrics['labels'].flatten(), val_metrics['predictions'].flatten()\n",
    "    \n",
    "    # 1. Confusion Matrix\n",
    "    cm = confusion_matrix(labels, preds > 0.5)\n",
    "    plt.figure(figsize=(8, 6)); sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])\n",
    "    plt.xlabel('Predicted'); plt.ylabel('Actual'); plt.title(f'Validation Confusion Matrix - Epoch {epoch}')\n",
    "    wandb.log({\"val_confusion_matrix\": wandb.Image(plt)}); plt.close()\n",
    "    \n",
    "    # 2. Prediction Distribution Histogram\n",
    "    plt.figure(figsize=(10, 6)); \n",
    "    sns.histplot(x=preds[labels==0], color='blue', alpha=0.5, label='Real Predictions', bins=30)\n",
    "    sns.histplot(x=preds[labels==1], color='red', alpha=0.5, label='Fake Predictions', bins=30)\n",
    "    plt.title(f'Validation Prediction Distribution - Epoch {epoch}'); plt.xlabel('Predicted Probability (of being Fake)'); plt.legend()\n",
    "    wandb.log({\"val_prediction_distribution\": wandb.Image(plt)}); plt.close()\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. MAIN FUNCTION\n",
    "# ==============================================================================\n",
    "def main():\n",
    "    config = Config()\n",
    "    device = torch.device(config.DEVICE)\n",
    "    wandb.init(project=config.WANDB_PROJECT_NAME, name=config.WANDB_RUN_NAME, config=vars(config))\n",
    "\n",
    "    # --- EXTENSIVE AUGMENTATION PIPELINE ---\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(), # Convert cv2 image to PIL Image for augmentations\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.2),\n",
    "        transforms.RandomResizedCrop(size=config.IMG_SIZE, scale=(0.8, 1.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Validation and test sets should NOT have augmentations\n",
    "    eval_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(config.IMG_SIZE), # Just resize, no cropping\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    print(\"Loading and splitting dataset with extensive augmentations...\")\n",
    "    dataset_for_train = DeepfakeDataset(root_dir=config.PREPROCESSED_DATA_DIR, transform=train_transform)\n",
    "    dataset_for_eval = DeepfakeDataset(root_dir=config.PREPROCESSED_DATA_DIR, transform=eval_transform)\n",
    "    \n",
    "    train_size, val_size = int(0.8 * len(dataset_for_train)), int(0.1 * len(dataset_for_train))\n",
    "    test_size = len(dataset_for_train) - train_size - val_size\n",
    "    indices = torch.randperm(len(dataset_for_train)).tolist()\n",
    "\n",
    "    train_dataset = torch.utils.data.Subset(dataset_for_train, indices[:train_size])\n",
    "    val_dataset = torch.utils.data.Subset(dataset_for_eval, indices[train_size:train_size + val_size])\n",
    "    test_dataset = torch.utils.data.Subset(dataset_for_eval, indices[train_size + val_size:])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    print(f\"Data split -> Train: {len(train_dataset)}, Validation: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "    model = LIDeepDet(config.VIT_MODEL_NAME, config.EMBED_DIM).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.LEARNING_RATE, weight_decay=1e-5)\n",
    "    pos_weight = torch.tensor(dataset_for_train.num_real / dataset_for_train.num_fake if dataset_for_train.num_fake > 0 else 1, dtype=torch.float32).to(device)\n",
    "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    best_val_auc = 0.0\n",
    "    for epoch in range(config.EPOCHS):\n",
    "        train_metrics = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "        val_metrics = evaluate(model, val_loader, loss_fn, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{config.EPOCHS} -> Train Loss: {train_metrics['loss']:.4f}, Train Acc: {train_metrics['accuracy']:.4f} | Val Loss: {val_metrics['loss']:.4f}, Val Acc: {val_metrics['accuracy']:.4f}\")\n",
    "        \n",
    "        wandb_logs = {\"epoch\": epoch + 1}\n",
    "        for key, val in train_metrics.items(): wandb_logs[f\"train_{key}\"] = val\n",
    "        for key, val in val_metrics.items(): \n",
    "            if key not in ['predictions', 'labels']: wandb_logs[f\"val_{key}\"] = val\n",
    "        wandb.log(wandb_logs)\n",
    "        \n",
    "        log_diagnostic_plots(val_metrics, epoch + 1)\n",
    "        \n",
    "        if val_metrics['loss'] < best_val_loss:\n",
    "            best_val_loss = val_metrics['loss']\n",
    "            print(f\"  -> New best model found with Val Loss: {best_val_loss:.4f}. Saving lean checkpoint...\")\n",
    "            \n",
    "            best_model_path = os.path.join(config.OUTPUT_DIR, 'best_checkpoint.pth')\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            \n",
    "            artifact = wandb.Artifact(f'best-model-run-{wandb.run.id}', type='model')\n",
    "            artifact.add_file(best_model_path)\n",
    "            wandb.log_artifact(artifact)\n",
    "\n",
    "    # --- Final Test Evaluation Logic (Unchanged) ---\n",
    "    print(\"\\n--- Training Finished. Starting Final Evaluation on Test Set ---\")\n",
    "    best_model_path = os.path.join(config.OUTPUT_DIR, 'best_checkpoint.pth')\n",
    "    if os.path.exists(best_model_path):\n",
    "        final_model = LIDeepDet(config.VIT_MODEL_NAME, config.EMBED_DIM).to(device)\n",
    "        final_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "        test_metrics = evaluate(final_model, test_loader, loss_fn, device)\n",
    "        print(f\"Final Test Set Results -> Accuracy: {test_metrics['accuracy']:.4f}, AUC: {test_metrics['auc']:.4f}, F1: {test_metrics['f1_score']:.4f}\")\n",
    "        wandb.log({f\"test_{k}\": v for k, v in test_metrics.items() if k not in ['predictions', 'labels']})\n",
    "    else:\n",
    "        print(\"No best model checkpoint found.\")\n",
    "    \n",
    "    wandb.finish()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
